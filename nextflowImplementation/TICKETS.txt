18) the container works fine in the old centos 6 cluster with singularity 3.5. In the cluster with the new centos version there is a problem with typeset -fx (i.e. declare the bash function). The problem is related to what is described in this post: https://stackoverflow.com/questions/58317190/automatically-source-configuration-when-running-singularity-container-with-shel
The solution to make the container work on all systems would be to convert the bash functions into separate independent bash scripts

17) you need to remove the big sample.bed file including all the reads. if you do that you cannot -resume anymore. one solution would be to combine all the processes using the bed file (covPerBin, covPerGe, and delly) but seems unpractical. Test the afterScript process directive to remove this file

16) make a new module simply loading the samples chrMedianCoverage files and producing an heatmap with the depth of coverage of each chromosome in each sample. You can make it read also the medinGenomicCoverage files, so that you can normalize the chromosome coverage of each sample with that to generate also a normalized heatmap 

15)  make that you can click on the figure to see the zoom or download it

13) produce consensus sequences for the new repetitive elements identified in your paper, add the SIDER element or whatever element publicly available from NCBI Nuleotides (probably they all are available from there,, i.e. the TATE elements..) and generate a file that can be used as a library for repeatmasker and distributed with the pipeline


9) Test it on mac!

7) consider reducing the size of singularity image removing the source code pacage and libraries that are unneded after you built everything you need. See apt-get autoremove, clean and autoclean

6) if you use .join then the sampleId (used as index to join together the right channel) is not accessible from the process. The only way to get the sample id is though the file input.1. However, if you do that you will have anyway problems using publishDir params.resultDir/samples/$sampleId 
copying manually the result .html file to the params.resultDir/$sampleId is not good either becuase the params.resultDir is relative to the execution dir, and not easy to guess from inside the work/ dir.

- Workaround (Current): just copy the .html file in params.resultDir/reports/ instead that in params.resultDir/samples/$sampleId
- Workaround2 (not tested): use the implicitly defined variable "launchDir" together with params.resultDir
 
5) develop the sampleComparison master script in bash or R, processing the user input in singularity run or exec. Comparison modes:
-SNV tree (consider to implement this as a nexflow pipeline to run the bootsraps and the starting trees in parallel on the cluster, ask fred for that, but he recommends to do it just on one computer and run it maybe multicore). Use IQ-Tree, it is maximum-likelihood, and faster (<3000 samples is fine) and more user friendly than RAxML. Do not use Fastree2, the accuracy is not good enough. Using just variant positions is good enough to build correct tree topologies. The only thing is that the branch length is not informative (this numally reflects the number of variants per base). Otherwise you need to run multiple genome alignments with mafft (can be CPU challenging, then also computing the tree)
-bin/gene to CNVs (including CNV correlation map and network analysis)
-mash k-mer PCA distance (possibly add a gip process to generate the sketch files)
-mash k-mer screen to look for contaminants starting from short reads and the mash refsseq database

4) generate a report karioplote with all the analysis together

3) process the repeatMasker output with something like /pasteur/entites/HubBioIT/gio/apps/my_scripts/various/reduceRepeatMaskerMotifsComplexity.pl

2) you can improve the freebayes step by running it in multi-process fashion


